{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae5241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_squad(path):\n",
    "    # open JSON file and load intro dictionary\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    # initialize lists for contexts, questions, and answers\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    # iterate through all data in squad data\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                # check if we need to be extracting from 'answers' or 'plausible_answers'\n",
    "                if 'plausible_answers' in qa.keys():\n",
    "                    access = 'plausible_answers'\n",
    "                else:\n",
    "                    access = 'answers'\n",
    "                for answer in qa[access]:\n",
    "                    # append data to lists\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    # return formatted data lists\n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d93db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute our read SQuAD function for training and validation sets\n",
    "train_contexts_old, train_questions_old, train_answers_old = read_squad('squad/train-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4392ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test-indexes.json\", 'r') as testfile:\n",
    "    rand_i_test = json.load(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdfce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tokenized-features-final-ints.json\", 'r') as f:\n",
    "    features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c08a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad features\n",
    "for f in features:\n",
    "    f += [[0,0,0,0]] * (512 - len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf34329",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "for i in rand_i_test:\n",
    "    test_features.append(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134aaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_contexts = []\n",
    "test_questions = []\n",
    "test_answers = []\n",
    "\n",
    "for i in rand_i_test:\n",
    "    test_contexts.append(train_contexts_old[i])\n",
    "    test_questions.append(train_questions_old[i])\n",
    "    test_answers.append(train_answers_old[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e382f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f52d0",
   "metadata": {},
   "source": [
    "## Who Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95a231c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "who_contexts = []\n",
    "who_questions = []\n",
    "who_answers = []\n",
    "who_features = []\n",
    "\n",
    "for i in range(len(test_questions)):\n",
    "    q_words = test_questions[i].split(' ')\n",
    "    q_words_lower = [normalize_text(word) for word in q_words]\n",
    "    if q_words_lower[0] == 'who':\n",
    "        who_contexts.append(test_contexts[i])\n",
    "        who_questions.append(test_questions[i])\n",
    "        who_answers.append(test_answers[i])\n",
    "        who_features.append(test_features[i])\n",
    "    elif q_words_lower[0] != 'where' and q_words_lower[0] != 'what' and 'what' not in q_words_lower and 'whats' not in q_words_lower and 'whatre' not in q_words_lower and q_words_lower[0] != 'why' and q_words_lower[0] != 'how' and 'how' not in q_words_lower and 'who' in q_words_lower:\n",
    "        who_contexts.append(test_contexts[i])\n",
    "        who_questions.append(test_questions[i])\n",
    "        who_answers.append(test_answers[i])\n",
    "        who_features.append(test_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742ae41",
   "metadata": {},
   "source": [
    "## With Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d204277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    # loop through each answer-context pair\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        # gold_text refers to the answer we are expecting to find in context\n",
    "        gold_text = answer['text']\n",
    "        # we already know the start index\n",
    "        start_idx = answer['answer_start']\n",
    "        # and ideally this would be the end index...\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # ...however, sometimes squad answers are off by a character or two\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            # if the answer is not off :)\n",
    "            answer['answer_end'] = end_idx\n",
    "        else:\n",
    "            # this means the answer is off by 1-2 tokens\n",
    "            for n in [1, 2]:\n",
    "                if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                    answer['answer_start'] = start_idx - n\n",
    "                    answer['answer_end'] = end_idx - n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf94d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mahmadin/ENV/lib/python3.6/site-packages/OpenSSL/crypto.py:8: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography import utils, x509\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b163a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "t_dir = '/scratch/mahmadin/.cache/huggingface/transformers'\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased',cache_dir=t_dir)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True, add_pooling_layer=False, cache_dir=t_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55d8d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    # initialize lists to contain the token indices of answer start/end\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        # append start/end token position using char_to_token method\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        # end position cannot be found, char_to_token found space, so shift position until found\n",
    "        shift = 1\n",
    "        while end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
    "            shift += 1\n",
    "    # update our encodings object with the new token-based start/end positions\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "470732e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class QANetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QANetwork, self).__init__()\n",
    "        self.num_labels = 2\n",
    "        self.hidden_size = 768 + 4\n",
    "        self.bert = bert_model\n",
    "        self.qa_outputs = nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, start_positions=None, end_positions=None, features=None):\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            #output_attentions=output_attentions,         Include these later if needed\n",
    "            #output_hidden_states=output_hidden_states,\n",
    "            #return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        # Concatenate logits with features\n",
    "        sequence_output = torch.cat([sequence_output, features], 2)\n",
    "        \n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "        \n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "        \n",
    "        return total_loss, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff634778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, features):\n",
    "        self.encodings = encodings\n",
    "        self.features = features\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sub = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        sub['features'] = torch.tensor(self.features[idx])\n",
    "        return sub\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a29f0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963278f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup GPU/CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be87fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are heavily influenced by the HF squad_metrics.py script\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    inputs = tokenizer(truth, return_tensors='pt', add_special_tokens=False)\n",
    "    truth = tokenizer.decode(inputs['input_ids'][0])\n",
    "\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    inputs = tokenizer(truth, return_tensors='pt', add_special_tokens=False)\n",
    "    truth = tokenizer.decode(inputs['input_ids'][0])\n",
    "\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens),int(pred_tokens == truth_tokens),int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = collections.Counter(truth_tokens) & collections.Counter(pred_tokens)\n",
    "    num_same = sum(common_tokens.values())\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if num_same == 0:\n",
    "        return 0,0,0\n",
    "\n",
    "    prec = 1.0 * num_same / len(pred_tokens)\n",
    "    rec = 1.0 * num_same / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec), prec, rec\n",
    "\n",
    "def Jaccard_index(context,answer,prediction):\n",
    "\n",
    "    inputs = tokenizer(answer, return_tensors='pt', add_special_tokens=False)\n",
    "    gold_answer0 = tokenizer.decode(inputs['input_ids'][0])\n",
    "\n",
    "    inputs = tokenizer(context, return_tensors='pt', add_special_tokens=False)\n",
    "    context = tokenizer.decode(inputs['input_ids'][0])\n",
    "\n",
    "    prediction = normalize_text(prediction)\n",
    "    gold_answer0 = normalize_text(gold_answer0)\n",
    "\n",
    "    text=\" \".join(word_tokenize(context)).lower()\n",
    "    gold_answers=\" \".join(word_tokenize(gold_answer0)).lower()\n",
    "    prediction = \" \".join(word_tokenize(prediction)).lower()\n",
    "    if prediction=='':\n",
    "        pred_set=set()\n",
    "    else:\n",
    "        pred_start = text.find(prediction)\n",
    "        pred_end = len(text) - (text[::-1].find(prediction[::-1]))\n",
    "        pred_set = set(list(range(pred_start, pred_end)))\n",
    "        if pred_start==-1 or pred_end==-1:\n",
    "            pred_set=set()\n",
    "\n",
    "    if gold_answers=='':\n",
    "        gold_start = 0\n",
    "        gold_end = 0\n",
    "        gold_set=set()\n",
    "    else:\n",
    "        gold_start = text.find(gold_answers)\n",
    "        gold_end = len(text) - (text[::-1].find(gold_answers[::-1]))\n",
    "        # gold_start = example.answers[0]['answer_start']\n",
    "        # gold_end = example.answers[0]['answer_end']\n",
    "        gold_set = set(list(range(gold_start, gold_end)))\n",
    "        if gold_start==-1 or gold_end==-1:\n",
    "            gold_set=set()\n",
    "\n",
    "\n",
    "    intersection=gold_set.intersection(pred_set)\n",
    "    union=gold_set.union(pred_set)\n",
    "\n",
    "\n",
    "    intersection_list=list(intersection)\n",
    "    union_list=list(union)\n",
    "\n",
    "\n",
    "    intersection_list.sort()\n",
    "    union_list.sort()\n",
    "\n",
    "    if not intersection_list:\n",
    "        intersection_word=''\n",
    "    else:\n",
    "        intersection_word=text[intersection_list[0]:intersection_list[-1] + 1]\n",
    "    if not union_list:\n",
    "        union_words=''\n",
    "    else:\n",
    "        union_words=text[union_list[0]:union_list[-1]+1]\n",
    "\n",
    "    intersection_word_length=len(word_tokenize(intersection_word))\n",
    "    union_word_length=len(word_tokenize(union_words))\n",
    "\n",
    "    if intersection_word_length==0 and union_word_length==0:\n",
    "        JI=1\n",
    "    else:\n",
    "        JI=intersection_word_length/union_word_length\n",
    "\n",
    "    return JI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "019bc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_end_idx(who_answers, who_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33cdfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "who_encodings = tokenizer(who_contexts, who_questions, max_length = 512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "842c763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_token_positions(who_encodings, who_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c45e51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build datasets for both our training and validation sets\n",
    "who_dataset = MyDataset(who_encodings, who_features)\n",
    "\n",
    "# initialize data loader for training data\n",
    "who_loader = DataLoader(who_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2854ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:27<00:00,  2.89it/s]\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "model_path = '/scratch/mahmadin/models/bert-squad-with-features-90-train-set'\n",
    "qa_model = nn.DataParallel(QANetwork().to(device))\n",
    "qa_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# switch model out of training mode\n",
    "qa_model.eval()\n",
    "\n",
    "\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "JI_scores = []\n",
    "\n",
    "# loop through batches\n",
    "for batch in tqdm(who_loader):\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = qa_model(input_ids, attention_mask, token_type_ids, features=batch_features)\n",
    "        \n",
    "        answer_start_index = outputs[1].argmax(axis=1)\n",
    "        answer_end_index = outputs[2].argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            predict_answer_tokens = input_ids[i, answer_start_index[i] : answer_end_index[i] + 1]\n",
    "            predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "            true_answer_tokens = input_ids[i, start_true[i] : end_true[i] + 1]\n",
    "            true_answer = tokenizer.decode(true_answer_tokens)\n",
    "            \n",
    "            em_scores.append(compute_exact_match(predicted_answer, true_answer))\n",
    "\n",
    "            scores = (compute_f1(predicted_answer, true_answer))\n",
    "            f1_scores.append(scores[0])\n",
    "            precision_scores.append(scores[1])\n",
    "            recall_scores.append(scores[2])\n",
    "            \n",
    "            context_tokens = input_ids[i, 0 : token_type_ids[i].tolist().index(1)]\n",
    "            context = tokenizer.decode(context_tokens)\n",
    "\n",
    "            JI_scores.append(Jaccard_index(context, true_answer, predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdda6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1417bb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:82.58057632856473\n",
      "Precision:83.45828648549282\n",
      "Recall:85.01524117595957\n",
      "Exact Match:76.54417513682564\n",
      "Jaccard Index:81.2685106171861\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1_score:{Average(f1_scores)*100}\")\n",
    "print(f\"Precision:{Average(precision_scores)*100}\")\n",
    "print(f\"Recall:{Average(recall_scores)*100}\")\n",
    "print(f\"Exact Match:{Average(em_scores)*100}\")\n",
    "print(f\"Jaccard Index:{Average(JI_scores)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34d829",
   "metadata": {},
   "source": [
    "## Without the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbe98f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class QANetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QANetwork, self).__init__()\n",
    "        self.num_labels = 2\n",
    "        self.hidden_size = 768 \n",
    "        self.bert = bert_model\n",
    "        self.qa_outputs = nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, start_positions=None, end_positions=None, features=None):\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            #output_attentions=output_attentions,         Include these later if needed\n",
    "            #output_hidden_states=output_hidden_states,\n",
    "            #return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        # Concatenate logits with features\n",
    "        if features is not None:\n",
    "            sequence_output = torch.cat([sequence_output, features], 2)\n",
    "        \n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "        \n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "        \n",
    "        return total_loss, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b60ed99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sub = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return sub\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d09f5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build datasets for both our training and validation sets\n",
    "who_dataset = MyDataset(who_encodings)\n",
    "\n",
    "# initialize data loader for training data\n",
    "who_loader = DataLoader(who_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f26b345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:17<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "model_path = '/scratch/mahmadin/models/bert-squad-without-features-90-train-set'\n",
    "qa_model_no_features = nn.DataParallel(QANetwork().to(device))\n",
    "qa_model_no_features.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# switch model out of training mode\n",
    "qa_model_no_features.eval()\n",
    "\n",
    "\n",
    "em_scores_without_features = []\n",
    "f1_scores_without_features = []\n",
    "precision_scores_without_features = []\n",
    "recall_scores_without_features = []\n",
    "JI_scores_without_features = []\n",
    "\n",
    "# loop through batches\n",
    "for batch in tqdm(who_loader):\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        #batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = qa_model_no_features(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        answer_start_index = outputs[1].argmax(axis=1)\n",
    "        answer_end_index = outputs[2].argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            predict_answer_tokens = input_ids[i, answer_start_index[i] : answer_end_index[i] + 1]\n",
    "            predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "            true_answer_tokens = input_ids[i, start_true[i] : end_true[i] + 1]\n",
    "            true_answer = tokenizer.decode(true_answer_tokens)\n",
    "            \n",
    "            em_scores_without_features.append(compute_exact_match(predicted_answer, true_answer))\n",
    "\n",
    "            scores = (compute_f1(predicted_answer, true_answer))\n",
    "            f1_scores_without_features.append(scores[0])\n",
    "            precision_scores_without_features.append(scores[1])\n",
    "            recall_scores_without_features.append(scores[2])\n",
    "            \n",
    "            context_tokens = input_ids[i, 0 : token_type_ids[i].tolist().index(1)]\n",
    "            context = tokenizer.decode(context_tokens)\n",
    "\n",
    "            JI_scores_without_features.append(Jaccard_index(context, true_answer, predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca0ea6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:81.55547925333526\n",
      "Precision:82.02992666271932\n",
      "Recall:85.58418454689668\n",
      "Exact Match:74.90226739640345\n",
      "Jaccard Index:79.95720078643681\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1_score:{Average(f1_scores_without_features)*100}\")\n",
    "print(f\"Precision:{Average(precision_scores_without_features)*100}\")\n",
    "print(f\"Recall:{Average(recall_scores_without_features)*100}\")\n",
    "print(f\"Exact Match:{Average(em_scores_without_features)*100}\")\n",
    "print(f\"Jaccard Index:{Average(JI_scores_without_features)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699b0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
