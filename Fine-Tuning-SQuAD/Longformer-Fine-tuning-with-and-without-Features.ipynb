{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae5241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_squad(path):\n",
    "    # open JSON file and load intro dictionary\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    # initialize lists for contexts, questions, and answers\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    # iterate through all data in squad data\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                # check if we need to be extracting from 'answers' or 'plausible_answers'\n",
    "                if 'plausible_answers' in qa.keys():\n",
    "                    access = 'plausible_answers'\n",
    "                else:\n",
    "                    access = 'answers'\n",
    "                for answer in qa[access]:\n",
    "                    # append data to lists\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    # return formatted data lists\n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d93db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute our read SQuAD function for training and validation sets\n",
    "train_contexts_old, train_questions_old, train_answers_old = read_squad('squad/train-v2.0.json')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40c1fe4d",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "# input list\n",
    "inputList = [i for i in range(0,130319)]\n",
    "# removing repeating elements from the list using the set() function\n",
    "resultSet=set(inputList)\n",
    "# converting the set into a list(now the list has only unique elements)\n",
    "uniqueList =list(resultSet)\n",
    "\n",
    "rand_i_train = random.sample(uniqueList, 117287)\n",
    "rand_i_test = [i for i in uniqueList if i not in rand_i_train]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98f01d30",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "with open(\"train-indexes.json\", 'w') as trainfile:\n",
    "    json.dump(rand_i_train, trainfile, indent=2)\n",
    "    \n",
    "with open(\"test-indexes.json\", 'w') as testfile:\n",
    "    json.dump(rand_i_test, testfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4392ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train-indexes.json\", 'r') as trainfile:\n",
    "    rand_i_train = json.load(trainfile)\n",
    "    \n",
    "with open(\"test-indexes.json\", 'r') as testfile:\n",
    "    rand_i_test = json.load(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84133ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use 90% for train\n",
    "train_contexts = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "\n",
    "for i in rand_i_train:\n",
    "    train_contexts.append(train_contexts_old[i])\n",
    "    train_questions.append(train_questions_old[i])\n",
    "    train_answers.append(train_answers_old[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134aaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_contexts = []\n",
    "test_questions = []\n",
    "test_answers = []\n",
    "\n",
    "for i in rand_i_test:\n",
    "    test_contexts.append(train_contexts_old[i])\n",
    "    test_questions.append(train_questions_old[i])\n",
    "    test_answers.append(train_answers_old[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d204277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    # loop through each answer-context pair\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        # gold_text refers to the answer we are expecting to find in context\n",
    "        gold_text = answer['text']\n",
    "        # we already know the start index\n",
    "        start_idx = answer['answer_start']\n",
    "        # and ideally this would be the end index...\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # ...however, sometimes squad answers are off by a character or two\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            # if the answer is not off :)\n",
    "            answer['answer_end'] = end_idx\n",
    "        else:\n",
    "            # this means the answer is off by 1-2 tokens\n",
    "            for n in [1, 2]:\n",
    "                if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                    answer['answer_start'] = start_idx - n\n",
    "                    answer['answer_end'] = end_idx - n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462b665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and apply the function to our two answer lists\n",
    "add_end_idx(train_answers, train_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf94d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizerFast, LongformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b163a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'longformer.pooler.dense.bias', 'longformer.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "t_dir = '/scratch/mahmadin/.cache/huggingface/transformers'\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096',cache_dir=t_dir)\n",
    "longformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096', add_pooling_layer=False, return_dict=True, cache_dir=t_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1305fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "train_encodings = tokenizer(train_contexts, train_questions, max_length=512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55d8d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    # initialize lists to contain the token indices of answer start/end\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        # append start/end token position using char_to_token method\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        # end position cannot be found, char_to_token found space, so shift position until found\n",
    "        shift = 1\n",
    "        while end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
    "            shift += 1\n",
    "    # update our encodings object with the new token-based start/end positions\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95557db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to our data\n",
    "add_token_positions(train_encodings, train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab8285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d56ab6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerModel(\n",
       "  (embeddings): LongformerEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): LongformerEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbdfce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tokenized-features-final-ints.json\", 'r') as f:\n",
    "    features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31c48296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the last 3 features (removing one of the features each time)\n",
    "for f in features:\n",
    "    for each in f:\n",
    "        each.pop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c1e5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad features (In case of poping)\n",
    "for f in features:\n",
    "    f += [[0,0,0]] * (512 - len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91c08a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad features\n",
    "for f in features:\n",
    "    f += [[0,0,0,0]] * (512 - len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c760052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use 90% of features\n",
    "train_features = []\n",
    "for i in rand_i_train:\n",
    "    train_features.append(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddf34329",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "for i in rand_i_test:\n",
    "    test_features.append(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca83eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class QANetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QANetwork, self).__init__()\n",
    "        self.num_labels = 2\n",
    "        \n",
    "        # with 3 features (linear layer)\n",
    "        self.hidden_size = 768 + 3\n",
    "        \n",
    "        # with 4 features\n",
    "        #self.hidden_size = 768 + 4\n",
    "        \n",
    "        self.longformer = longformer_model\n",
    "        self.qa_outputs = nn.Linear(self.hidden_size, self.num_labels)\n",
    "        \n",
    "        # for having 3 features (linear layer)\n",
    "        #self.features_linear_layer = nn.Linear(4, 3)\n",
    "        #self.features_relu = nn.ReLU()\n",
    "        #self.features_lstm = nn.LSTM(input_size=4, hidden_size=3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None, features=None):\n",
    "        \n",
    "        outputs = self.longformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #token_type_ids=token_type_ids,\n",
    "            #output_attentions=output_attentions,         Include these later if needed\n",
    "            #output_hidden_states=output_hidden_states,\n",
    "            #return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        # for having 3 features (linear layer)\n",
    "        #features_torch = torch.tensor(features)\n",
    "        #features_torch = features_torch.type(torch.float)\n",
    "        #features_linear_output = self.features_linear_layer(features_torch)\n",
    "        #features_output = self.features_relu(features_linear_output)\n",
    "        #features_output, (hn, cn) = self.features_lstm(features_torch)\n",
    "        #sequence_output = torch.cat([sequence_output, features_output], 2)\n",
    "        \n",
    "        # for having 4 features (without linear layer)\n",
    "        sequence_output = torch.cat([sequence_output, features], 2)\n",
    "        \n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "        \n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "        \n",
    "        return total_loss, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff634778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, features):\n",
    "        self.encodings = encodings\n",
    "        self.features = features\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sub = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        sub['features'] = torch.tensor(self.features[idx])\n",
    "        return sub\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c5164de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build datasets for both our training and validation sets\n",
    "train_dataset = MyDataset(train_encodings, train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a29f0789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mahmadin/ENV/lib/python3.6/site-packages/OpenSSL/crypto.py:8: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography import utils, x509\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efec35e8",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=1,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "963278f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup GPU/CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "816533ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = nn.DataParallel(QANetwork().to(device))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "239fba13",
   "metadata": {},
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57dd3cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mahmadin/ENV/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
    "optim = AdamW(qa_model.parameters(), lr=5e-5)\n",
    "\n",
    "# initialize data loader for training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01309a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7331 [00:00<?, ?it/s]/scratch/mahmadin/ENV/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Epoch 0: 100%|██████████| 7331/7331 [2:35:16<00:00,  1.27s/it, loss=5.46]  \n",
      "Epoch 1: 100%|██████████| 7331/7331 [2:58:57<00:00,  1.46s/it, loss=3.79]   \n",
      "Epoch 2: 100%|██████████| 7331/7331 [3:04:09<00:00,  1.51s/it, loss=1.68]   \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    # set model to train mode\n",
    "    qa_model.train()\n",
    "    # setup loop (we use tqdm for the progress bar)\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all the tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        outputs = qa_model(input_ids, attention_mask, start_positions, end_positions, batch_features)\n",
    "        # extract loss\n",
    "        loss = outputs[0]  # 0: total loss, 1: start logits, 2: end logits\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.sum().backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be87fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are heavily influenced by the HF squad_metrics.py script\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    inputs = tokenizer(truth, return_tensors='pt', add_special_tokens=False)\n",
    "    truth = tokenizer.decode(inputs['input_ids'][0])\n",
    "\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    inputs = tokenizer(truth, return_tensors='pt', add_special_tokens=False)\n",
    "    truth = tokenizer.decode(inputs['input_ids'][0])\n",
    "\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens),int(pred_tokens == truth_tokens),int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = collections.Counter(truth_tokens) & collections.Counter(pred_tokens)\n",
    "    num_same = sum(common_tokens.values())\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if num_same == 0:\n",
    "        return 0,0,0\n",
    "\n",
    "    prec = 1.0 * num_same / len(pred_tokens)\n",
    "    rec = 1.0 * num_same / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec), prec, rec\n",
    "\n",
    "def Jaccard_index(context,answer,prediction):\n",
    "\n",
    "    inputs = tokenizer(answer, return_tensors='pt', add_special_tokens=False)\n",
    "    gold_answer0 = tokenizer.decode(inputs['input_ids'][0])\n",
    "\n",
    "    inputs = tokenizer(context, return_tensors='pt', add_special_tokens=False)\n",
    "    context = tokenizer.decode(inputs['input_ids'][0])\n",
    "\n",
    "    prediction = normalize_text(prediction)\n",
    "    gold_answer0 = normalize_text(gold_answer0)\n",
    "\n",
    "    text=\" \".join(word_tokenize(context)).lower()\n",
    "    gold_answers=\" \".join(word_tokenize(gold_answer0)).lower()\n",
    "    prediction = \" \".join(word_tokenize(prediction)).lower()\n",
    "    if prediction=='':\n",
    "        pred_set=set()\n",
    "    else:\n",
    "        pred_start = text.find(prediction)\n",
    "        pred_end = len(text) - (text[::-1].find(prediction[::-1]))\n",
    "        pred_set = set(list(range(pred_start, pred_end)))\n",
    "        if pred_start==-1 or pred_end==-1:\n",
    "            pred_set=set()\n",
    "\n",
    "    if gold_answers=='':\n",
    "        gold_start = 0\n",
    "        gold_end = 0\n",
    "        gold_set=set()\n",
    "    else:\n",
    "        gold_start = text.find(gold_answers)\n",
    "        gold_end = len(text) - (text[::-1].find(gold_answers[::-1]))\n",
    "        # gold_start = example.answers[0]['answer_start']\n",
    "        # gold_end = example.answers[0]['answer_end']\n",
    "        gold_set = set(list(range(gold_start, gold_end)))\n",
    "        if gold_start==-1 or gold_end==-1:\n",
    "            gold_set=set()\n",
    "\n",
    "\n",
    "    intersection=gold_set.intersection(pred_set)\n",
    "    union=gold_set.union(pred_set)\n",
    "\n",
    "\n",
    "    intersection_list=list(intersection)\n",
    "    union_list=list(union)\n",
    "\n",
    "\n",
    "    intersection_list.sort()\n",
    "    union_list.sort()\n",
    "\n",
    "    if not intersection_list:\n",
    "        intersection_word=''\n",
    "    else:\n",
    "        intersection_word=text[intersection_list[0]:intersection_list[-1] + 1]\n",
    "    if not union_list:\n",
    "        union_words=''\n",
    "    else:\n",
    "        union_words=text[union_list[0]:union_list[-1]+1]\n",
    "\n",
    "    intersection_word_length=len(word_tokenize(intersection_word))\n",
    "    union_word_length=len(word_tokenize(union_words))\n",
    "\n",
    "    if intersection_word_length==0 and union_word_length==0:\n",
    "        JI=1\n",
    "    else:\n",
    "        JI=intersection_word_length/union_word_length\n",
    "\n",
    "    return JI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde289b",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "276f5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longformer-squad with 4 features\n",
    "model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set'\n",
    "#Save the model\n",
    "torch.save(qa_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a603028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longformer-squad with 3 features (linear layer)\n",
    "model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-with-features-linear-layer'\n",
    "#Save the model\n",
    "torch.save(qa_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40215b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longformer-squad with 3 features (LSTM)\n",
    "model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-with-features-lstm-layer'\n",
    "#Save the model\n",
    "torch.save(qa_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c8a3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longformer-squad without NER\n",
    "model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-without-NER-without-nn'\n",
    "#Save the model\n",
    "torch.save(qa_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61011336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longformer-squad without POS\n",
    "model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-without-POS-without-nn'\n",
    "#Save the model\n",
    "torch.save(qa_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1024d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longformer-squad without DEP\n",
    "model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-without-DEP-without-nn'\n",
    "#Save the model\n",
    "torch.save(qa_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a72389",
   "metadata": {},
   "source": [
    "## Use 90% of training set to train and 10% to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "019bc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_end_idx(test_answers, test_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33cdfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "test_encodings = tokenizer(test_contexts, test_questions, max_length = 512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "842c763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_token_positions(test_encodings, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c45e51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build datasets for both our training and validation sets\n",
    "test_dataset = MyDataset(test_encodings, test_features)\n",
    "\n",
    "# initialize data loader for training data\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1126fe",
   "metadata": {},
   "source": [
    "## With 4 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2854ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 815/815 [05:15<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# longformer-squad with 4 features\n",
    "\n",
    "#Load the model\n",
    "#model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set'\n",
    "#qa_model = nn.DataParallel(QANetwork().to(device))\n",
    "#qa_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# switch model out of training mode\n",
    "qa_model.eval()\n",
    "\n",
    "\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "JI_scores = []\n",
    "\n",
    "# loop through batches\n",
    "for batch in tqdm(test_loader):\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = qa_model(input_ids, attention_mask, features=batch_features)\n",
    "        \n",
    "        answer_start_index = outputs[1].argmax(axis=1)\n",
    "        answer_end_index = outputs[2].argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            predict_answer_tokens = input_ids[i, answer_start_index[i] : answer_end_index[i] + 1]\n",
    "            predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "            true_answer_tokens = input_ids[i, start_true[i] : end_true[i] + 1]\n",
    "            true_answer = tokenizer.decode(true_answer_tokens)\n",
    "            \n",
    "            em_scores.append(compute_exact_match(predicted_answer, true_answer))\n",
    "\n",
    "            scores = (compute_f1(predicted_answer, true_answer))\n",
    "            f1_scores.append(scores[0])\n",
    "            precision_scores.append(scores[1])\n",
    "            recall_scores.append(scores[2])\n",
    "            \n",
    "            context_tokens = input_ids[i, 1 : input_ids[i].tolist().index(2)]\n",
    "            context = tokenizer.decode(context_tokens)\n",
    "\n",
    "            JI_scores.append(Jaccard_index(context, true_answer, predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdda6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f494808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:81.11701040180475\n",
      "Precision:83.33228319586577\n",
      "Recall:83.37108555104507\n",
      "Exact Match:69.08379373848987\n",
      "Jaccard Index:79.73050379996714\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1_score:{Average(f1_scores)*100}\")\n",
    "print(f\"Precision:{Average(precision_scores)*100}\")\n",
    "print(f\"Recall:{Average(recall_scores)*100}\")\n",
    "print(f\"Exact Match:{Average(em_scores)*100}\")\n",
    "print(f\"Jaccard Index:{Average(JI_scores)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18826a1a",
   "metadata": {},
   "source": [
    "## With 3 Features Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "421ff7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/815 [00:00<?, ?it/s]/scratch/mahmadin/ENV/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 815/815 [05:15<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# longformer-squad with 3 features (linear layer)\n",
    "\n",
    "#Load the model\n",
    "#model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-with-features-linear-layer'\n",
    "#qa_model = nn.DataParallel(QANetwork().to(device))\n",
    "#qa_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# switch model out of training mode\n",
    "qa_model.eval()\n",
    "\n",
    "\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "JI_scores = []\n",
    "\n",
    "# loop through batches\n",
    "for batch in tqdm(test_loader):\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = qa_model(input_ids, attention_mask, features=batch_features)\n",
    "        \n",
    "        answer_start_index = outputs[1].argmax(axis=1)\n",
    "        answer_end_index = outputs[2].argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            predict_answer_tokens = input_ids[i, answer_start_index[i] : answer_end_index[i] + 1]\n",
    "            predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "            true_answer_tokens = input_ids[i, start_true[i] : end_true[i] + 1]\n",
    "            true_answer = tokenizer.decode(true_answer_tokens)\n",
    "            \n",
    "            em_scores.append(compute_exact_match(predicted_answer, true_answer))\n",
    "\n",
    "            scores = (compute_f1(predicted_answer, true_answer))\n",
    "            f1_scores.append(scores[0])\n",
    "            precision_scores.append(scores[1])\n",
    "            recall_scores.append(scores[2])\n",
    "            \n",
    "            context_tokens = input_ids[i, 1 : input_ids[i].tolist().index(2)]\n",
    "            context = tokenizer.decode(context_tokens)\n",
    "\n",
    "            JI_scores.append(Jaccard_index(context, true_answer, predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a015031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "240aaf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:80.29707018959142\n",
      "Precision:82.47405691525381\n",
      "Recall:82.90850565721856\n",
      "Exact Match:68.21669736034377\n",
      "Jaccard Index:78.95786670231082\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1_score:{Average(f1_scores)*100}\")\n",
    "print(f\"Precision:{Average(precision_scores)*100}\")\n",
    "print(f\"Recall:{Average(recall_scores)*100}\")\n",
    "print(f\"Exact Match:{Average(em_scores)*100}\")\n",
    "print(f\"Jaccard Index:{Average(JI_scores)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af56af",
   "metadata": {},
   "source": [
    "## With 3 Features LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb6fa5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/815 [00:00<?, ?it/s]/scratch/mahmadin/ENV/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/scratch/mahmadin/ENV/lib/python3.6/site-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "100%|██████████| 815/815 [03:57<00:00,  3.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# longformer-squad with 3 features (LSTM)\n",
    "\n",
    "#Load the model\n",
    "#model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-with-features-lstm-layer'\n",
    "#qa_model = nn.DataParallel(QANetwork().to(device))\n",
    "#qa_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# switch model out of training mode\n",
    "qa_model.eval()\n",
    "\n",
    "\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "JI_scores = []\n",
    "\n",
    "# loop through batches\n",
    "for batch in tqdm(test_loader):\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = qa_model(input_ids, attention_mask, features=batch_features)\n",
    "        \n",
    "        answer_start_index = outputs[1].argmax(axis=1)\n",
    "        answer_end_index = outputs[2].argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            predict_answer_tokens = input_ids[i, answer_start_index[i] : answer_end_index[i] + 1]\n",
    "            predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "            true_answer_tokens = input_ids[i, start_true[i] : end_true[i] + 1]\n",
    "            true_answer = tokenizer.decode(true_answer_tokens)\n",
    "            \n",
    "            em_scores.append(compute_exact_match(predicted_answer, true_answer))\n",
    "\n",
    "            scores = (compute_f1(predicted_answer, true_answer))\n",
    "            f1_scores.append(scores[0])\n",
    "            precision_scores.append(scores[1])\n",
    "            recall_scores.append(scores[2])\n",
    "            \n",
    "            context_tokens = input_ids[i, 1 : input_ids[i].tolist().index(2)]\n",
    "            context = tokenizer.decode(context_tokens)\n",
    "\n",
    "            JI_scores.append(Jaccard_index(context, true_answer, predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bbe3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8ed369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:81.40162037553972\n",
      "Precision:82.86560342625857\n",
      "Recall:84.33357683046255\n",
      "Exact Match:69.29864947820748\n",
      "Jaccard Index:80.11588914959358\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1_score:{Average(f1_scores)*100}\")\n",
    "print(f\"Precision:{Average(precision_scores)*100}\")\n",
    "print(f\"Recall:{Average(recall_scores)*100}\")\n",
    "print(f\"Exact Match:{Average(em_scores)*100}\")\n",
    "print(f\"Jaccard Index:{Average(JI_scores)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922840e",
   "metadata": {},
   "source": [
    "## Without NER Without NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad5ff8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 815/815 [08:19<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# longformer-squad without NER without nn\n",
    "\n",
    "#Load the model\n",
    "#model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-without-NER-without-nn'\n",
    "#qa_model = nn.DataParallel(QANetwork().to(device))\n",
    "#qa_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# switch model out of training mode\n",
    "qa_model.eval()\n",
    "\n",
    "\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "JI_scores = []\n",
    "\n",
    "# loop through batches\n",
    "for batch in tqdm(test_loader):\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = qa_model(input_ids, attention_mask, features=batch_features)\n",
    "        \n",
    "        answer_start_index = outputs[1].argmax(axis=1)\n",
    "        answer_end_index = outputs[2].argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            predict_answer_tokens = input_ids[i, answer_start_index[i] : answer_end_index[i] + 1]\n",
    "            predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "            true_answer_tokens = input_ids[i, start_true[i] : end_true[i] + 1]\n",
    "            true_answer = tokenizer.decode(true_answer_tokens)\n",
    "            \n",
    "            em_scores.append(compute_exact_match(predicted_answer, true_answer))\n",
    "\n",
    "            scores = (compute_f1(predicted_answer, true_answer))\n",
    "            f1_scores.append(scores[0])\n",
    "            precision_scores.append(scores[1])\n",
    "            recall_scores.append(scores[2])\n",
    "            \n",
    "            context_tokens = input_ids[i, 1 : input_ids[i].tolist().index(2)]\n",
    "            context = tokenizer.decode(context_tokens)\n",
    "\n",
    "            JI_scores.append(Jaccard_index(context, true_answer, predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be323a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20ef981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:80.07841620210333\n",
      "Precision:82.01973981508998\n",
      "Recall:82.92920951205754\n",
      "Exact Match:68.2243707796194\n",
      "Jaccard Index:79.03660358664233\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1_score:{Average(f1_scores)*100}\")\n",
    "print(f\"Precision:{Average(precision_scores)*100}\")\n",
    "print(f\"Recall:{Average(recall_scores)*100}\")\n",
    "print(f\"Exact Match:{Average(em_scores)*100}\")\n",
    "print(f\"Jaccard Index:{Average(JI_scores)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be832a",
   "metadata": {},
   "source": [
    "## Without POS Without NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52660cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 815/815 [09:36<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# longformer-squad without POS without nn\n",
    "\n",
    "#Load the model\n",
    "#model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-without-POS-without-nn'\n",
    "#qa_model = nn.DataParallel(QANetwork().to(device))\n",
    "#qa_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# switch model out of training mode\n",
    "qa_model.eval()\n",
    "\n",
    "\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "JI_scores = []\n",
    "\n",
    "# loop through batches\n",
    "for batch in tqdm(test_loader):\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = qa_model(input_ids, attention_mask, features=batch_features)\n",
    "        \n",
    "        answer_start_index = outputs[1].argmax(axis=1)\n",
    "        answer_end_index = outputs[2].argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            predict_answer_tokens = input_ids[i, answer_start_index[i] : answer_end_index[i] + 1]\n",
    "            predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "            true_answer_tokens = input_ids[i, start_true[i] : end_true[i] + 1]\n",
    "            true_answer = tokenizer.decode(true_answer_tokens)\n",
    "            \n",
    "            em_scores.append(compute_exact_match(predicted_answer, true_answer))\n",
    "\n",
    "            scores = (compute_f1(predicted_answer, true_answer))\n",
    "            f1_scores.append(scores[0])\n",
    "            precision_scores.append(scores[1])\n",
    "            recall_scores.append(scores[2])\n",
    "            \n",
    "            context_tokens = input_ids[i, 1 : input_ids[i].tolist().index(2)]\n",
    "            context = tokenizer.decode(context_tokens)\n",
    "\n",
    "            JI_scores.append(Jaccard_index(context, true_answer, predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d68e9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58491f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:80.54945161964143\n",
      "Precision:83.30231584171175\n",
      "Recall:82.28255870521193\n",
      "Exact Match:68.70779619398404\n",
      "Jaccard Index:79.40271648208133\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1_score:{Average(f1_scores)*100}\")\n",
    "print(f\"Precision:{Average(precision_scores)*100}\")\n",
    "print(f\"Recall:{Average(recall_scores)*100}\")\n",
    "print(f\"Exact Match:{Average(em_scores)*100}\")\n",
    "print(f\"Jaccard Index:{Average(JI_scores)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f3d02",
   "metadata": {},
   "source": [
    "## Without DEP Without NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9215ce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 815/815 [15:47<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# longformer-squad without DEP without nn\n",
    "\n",
    "#Load the model\n",
    "#model_path = '/scratch/mahmadin/models/longformer-squad-with-features-90-train-set-without-DEP-without-nn'\n",
    "#qa_model = nn.DataParallel(QANetwork().to(device))\n",
    "#qa_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# switch model out of training mode\n",
    "qa_model.eval()\n",
    "\n",
    "\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "JI_scores = []\n",
    "\n",
    "# loop through batches\n",
    "for batch in tqdm(test_loader):\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = qa_model(input_ids, attention_mask, features=batch_features)\n",
    "        \n",
    "        answer_start_index = outputs[1].argmax(axis=1)\n",
    "        answer_end_index = outputs[2].argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            predict_answer_tokens = input_ids[i, answer_start_index[i] : answer_end_index[i] + 1]\n",
    "            predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "            true_answer_tokens = input_ids[i, start_true[i] : end_true[i] + 1]\n",
    "            true_answer = tokenizer.decode(true_answer_tokens)\n",
    "            \n",
    "            em_scores.append(compute_exact_match(predicted_answer, true_answer))\n",
    "\n",
    "            scores = (compute_f1(predicted_answer, true_answer))\n",
    "            f1_scores.append(scores[0])\n",
    "            precision_scores.append(scores[1])\n",
    "            recall_scores.append(scores[2])\n",
    "            \n",
    "            context_tokens = input_ids[i, 1 : input_ids[i].tolist().index(2)]\n",
    "            context = tokenizer.decode(context_tokens)\n",
    "\n",
    "            JI_scores.append(Jaccard_index(context, true_answer, predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3c05c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "846867e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:80.7786548160234\n",
      "Precision:83.97609926901785\n",
      "Recall:82.01158036987614\n",
      "Exact Match:68.8842848373235\n",
      "Jaccard Index:79.33696213130911\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1_score:{Average(f1_scores)*100}\")\n",
    "print(f\"Precision:{Average(precision_scores)*100}\")\n",
    "print(f\"Recall:{Average(recall_scores)*100}\")\n",
    "print(f\"Exact Match:{Average(em_scores)*100}\")\n",
    "print(f\"Jaccard Index:{Average(JI_scores)*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34d829",
   "metadata": {},
   "source": [
    "## Without the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbe98f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class QANetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QANetwork, self).__init__()\n",
    "        self.num_labels = 2\n",
    "        self.hidden_size = 768 \n",
    "        self.longformer = longformer_model\n",
    "        self.qa_outputs = nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None, features=None):\n",
    "        \n",
    "        outputs = self.longformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #token_type_ids=token_type_ids,\n",
    "            #output_attentions=output_attentions,         Include these later if needed\n",
    "            #output_hidden_states=output_hidden_states,\n",
    "            #return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        # Concatenate logits with features\n",
    "        if features is not None:\n",
    "            sequence_output = torch.cat([sequence_output, features], 2)\n",
    "        \n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "        \n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "        \n",
    "        return total_loss, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b60ed99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sub = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return sub\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d09f5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build datasets for both our training and validation sets\n",
    "train_dataset = MyDataset(train_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "961d2fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mahmadin/ENV/lib/python3.6/site-packages/OpenSSL/crypto.py:8: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography import utils, x509\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64a38017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup GPU/CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4076fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model_no_features = nn.DataParallel(QANetwork().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "676ef44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/mahmadin/ENV/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
    "optim = AdamW(qa_model_no_features.parameters(), lr=5e-5)\n",
    "\n",
    "# initialize data loader for training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf040a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7331 [00:00<?, ?it/s]/scratch/mahmadin/ENV/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Epoch 0: 100%|██████████| 7331/7331 [1:16:30<00:00,  1.60it/s, loss=2.31]\n",
      "Epoch 1: 100%|██████████| 7331/7331 [1:16:37<00:00,  1.59it/s, loss=3.16] \n",
      "Epoch 2: 100%|██████████| 7331/7331 [1:16:43<00:00,  1.59it/s, loss=3.04] \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    # set model to train mode\n",
    "    qa_model_no_features.train()\n",
    "    # setup loop (we use tqdm for the progress bar)\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all the tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        #batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        outputs = qa_model_no_features(input_ids, attention_mask, start_positions, end_positions)\n",
    "        # extract loss\n",
    "        loss = outputs[0]  # 0: total loss, 1: start logits, 2: end logits\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.sum().backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbc653",
   "metadata": {},
   "source": [
    "## Use 90% of training set to train and 10% to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22d2badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/scratch/mahmadin/models/longformer-squad-without-features-90-train-set'\n",
    "#Save the model\n",
    "torch.save(qa_model_no_features.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0578a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_end_idx(test_answers, test_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df06e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "test_encodings = tokenizer(test_contexts, test_questions, max_length = 512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67809543",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_token_positions(test_encodings, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f8e2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build datasets for both our training and validation sets\n",
    "test_dataset = MyDataset(test_encodings)\n",
    "\n",
    "# initialize data loader for training data\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "170e424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 815/815 [05:10<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "#model_path = '/scratch/mahmadin/models/longformer-squad-without-features-90-train-set'\n",
    "#qa_model_no_features = nn.DataParallel(QANetwork().to(device))\n",
    "#qa_model_no_features.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# switch model out of training mode\n",
    "qa_model_no_features.eval()\n",
    "\n",
    "\n",
    "em_scores_without_features = []\n",
    "f1_scores_without_features = []\n",
    "precision_scores_without_features = []\n",
    "recall_scores_without_features = []\n",
    "JI_scores_without_features = []\n",
    "\n",
    "# loop through batches\n",
    "for batch in tqdm(test_loader):\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #token_type_ids = batch['token_type_ids'].to(device)\n",
    "        #batch_features = batch['features'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = qa_model_no_features(input_ids, attention_mask)\n",
    "        \n",
    "        answer_start_index = outputs[1].argmax(axis=1)\n",
    "        answer_end_index = outputs[2].argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            predict_answer_tokens = input_ids[i, answer_start_index[i] : answer_end_index[i] + 1]\n",
    "            predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
    "            \n",
    "            true_answer_tokens = input_ids[i, start_true[i] : end_true[i] + 1]\n",
    "            true_answer = tokenizer.decode(true_answer_tokens)\n",
    "            \n",
    "            em_scores_without_features.append(compute_exact_match(predicted_answer, true_answer))\n",
    "\n",
    "            scores = (compute_f1(predicted_answer, true_answer))\n",
    "            f1_scores_without_features.append(scores[0])\n",
    "            precision_scores_without_features.append(scores[1])\n",
    "            recall_scores_without_features.append(scores[2])\n",
    "            \n",
    "            context_tokens = input_ids[i, 1 : input_ids[i].tolist().index(2)]\n",
    "            context = tokenizer.decode(context_tokens)\n",
    "\n",
    "            JI_scores_without_features.append(Jaccard_index(context, true_answer, predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fd87263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c2aeaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:80.74892141975855\n",
      "Precision:82.03876595563916\n",
      "Recall:84.19767936079747\n",
      "Exact Match:68.36249232658072\n",
      "Jaccard Index:79.38317262440603\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1_score:{Average(f1_scores_without_features)*100}\")\n",
    "print(f\"Precision:{Average(precision_scores_without_features)*100}\")\n",
    "print(f\"Recall:{Average(recall_scores_without_features)*100}\")\n",
    "print(f\"Exact Match:{Average(em_scores_without_features)*100}\")\n",
    "print(f\"Jaccard Index:{Average(JI_scores_without_features)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125360a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
